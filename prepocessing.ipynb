{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9211feda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# STOPWORDS + STEMMER\n",
    "stopwords = set(StopWordRemoverFactory().get_stop_words())\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "# ===== CLEANING LEVEL TINGGI =====\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    # buang URL\n",
    "    text = re.sub(r'http\\S+|www\\S+|pic\\S+', ' ', text)\n",
    "\n",
    "    # buang file gambar: jpg png jpeg webp\n",
    "    text = re.sub(r'\\b\\w+\\.(jpg|jpeg|png|webp|gif)\\b', ' ', text)\n",
    "\n",
    "    # buang kata scrap random: superjumbo, dccb, fde, aaab, dll\n",
    "    text = re.sub(r'\\b[a-z]{1,4}\\b', ' ', text)  # buang kata 1–4 huruf (noise)\n",
    "\n",
    "    # pisahkan kata sambung kacau: \"virusseolah\" → \"virus seolah\"\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'([a-z]+)([A-Z][a-z]+)', r'\\1 \\2', text)\n",
    "\n",
    "    # hanya huruf\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "    # collapse spasi\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ===== TOKENIZER FINAL =====\n",
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = text.split()\n",
    "\n",
    "    # remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "\n",
    "    # stemming\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    # remove leftover noise\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ===== PROCESS ALL FILES =====\n",
    "def process_corpus(raw_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    all_tokens = []\n",
    "    files = [f for f in os.listdir(raw_folder) if f.endswith(\".txt\")]\n",
    "\n",
    "    print(\"Total file ditemukan:\", len(files))\n",
    "    progress = tqdm(files, desc=\"Processing Documents\")\n",
    "\n",
    "    for fname in progress:\n",
    "        path = os.path.join(raw_folder, fname)\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        tokens = preprocess_text(text)\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "        out_path = os.path.join(output_folder, fname.replace(\".txt\", \".json\"))\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as o:\n",
    "            json.dump({\"tokens\": tokens}, o, indent=2)\n",
    "\n",
    "    vocabulary = sorted(set(all_tokens))\n",
    "    json.dump({\"vocabulary\": vocabulary}, open(\"dictionary.json\",\"w\"), indent=2)\n",
    "\n",
    "    print(\"\\n=== CLEANING DONE ===\")\n",
    "    print(\"Dokumen :\", len(files))\n",
    "    print(\"Vocab   :\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b33e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total file ditemukan: 412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Documents: 100%|██████████| 412/412 [08:46<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CLEANING DONE ===\n",
      "Dokumen : 412\n",
      "Vocab   : 7661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process_corpus(\"dataset_raw\", \"dataset_clean\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scifind)",
   "language": "python",
   "name": "scifind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
